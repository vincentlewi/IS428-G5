{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm.asyncio import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the csv files in the directory and concatenate them into a single dataframe\n",
    "dir = \"../datasets/private/raw\"\n",
    "\n",
    "private = pd.concat(\n",
    "    [pd.read_csv(os.path.join(dir, f)) for f in os.listdir(dir) if f.endswith(\".csv\")],\n",
    "    ignore_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the column names and convert the sale date to a datetime object\n",
    "private.columns = private.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "private[\"sale_date\"] = pd.to_datetime(\n",
    "    private[\"sale_date\"], format=\"%b-%y\"\n",
    ").dt.to_period(\"M\")\n",
    "\n",
    "# Create address column by concatenating project_name and street_name\n",
    "private[\"address\"] = private[\"project_name\"] + \" \" + private[\"street_name\"]\n",
    "private = private.drop(columns=[\"project_name\", \"street_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataframe to a csv file\n",
    "private.to_csv(\n",
    "    \"../datasets/private/private.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = private[\"address\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching data: 4573it [00:41, 110.00it/s]                          \n"
     ]
    }
   ],
   "source": [
    "# Geocode addresses using OneMap API\n",
    "url = \"https://www.onemap.gov.sg/api/common/elastic/\"\n",
    "\n",
    "\n",
    "async def get_hdb_data(address, session, progress_bar=None):\n",
    "    params = {\"searchVal\": address, \"returnGeom\": \"Y\", \"getAddrDetails\": \"Y\"}\n",
    "    try:\n",
    "        async with session.get(url, params=params) as response:\n",
    "            data = await response.json()\n",
    "            if data[\"found\"] != 0:\n",
    "                result = data[\"results\"][0]\n",
    "                if progress_bar:\n",
    "                    progress_bar.update(1)\n",
    "                return {**{\"QUERY\": address}, **result}\n",
    "            if progress_bar:\n",
    "                progress_bar.update(1)\n",
    "            return {\"QUERY\": address, \"found\": 0}\n",
    "    except Exception as e:\n",
    "        if progress_bar:\n",
    "            progress_bar.update(1)\n",
    "        return {\"QUERY\": address, \"ERROR\": \"REQUEST FAILED\"}\n",
    "\n",
    "\n",
    "async def main(addresses):\n",
    "    results = []\n",
    "    retry_addresses = addresses  # Initial set of addresses to try\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        progress_bar = tqdm(total=len(addresses), desc=\"Fetching data\")\n",
    "        while retry_addresses:\n",
    "            tasks = [\n",
    "                get_hdb_data(address, session, progress_bar)\n",
    "                for address in retry_addresses\n",
    "            ]\n",
    "            batch_results = await asyncio.gather(*tasks)\n",
    "\n",
    "            retry_addresses = (\n",
    "                []\n",
    "            )  # Reset retry_addresses list for the next iteration for any errors\n",
    "\n",
    "            for res in batch_results:\n",
    "                if res is not None:\n",
    "                    if (\n",
    "                        \"ERROR\" in res\n",
    "                    ):  # Check if the response is an error, without updating progress\n",
    "                        retry_addresses.append(res[\"QUERY\"])  # Add address for retry\n",
    "                    elif res.get(\"found\", 1) == 0:  # Address has no data but no error\n",
    "                        results.append(res)\n",
    "                    else:  # Success case\n",
    "                        results.append(res)\n",
    "        progress_bar.close()\n",
    "\n",
    "    if results:\n",
    "        # Convert list of dicts to DataFrame, omitting error entries for presentation\n",
    "        results_df = pd.DataFrame([res for res in results if \"ERROR\" not in res])\n",
    "        return results_df.drop(columns=\"found\")\n",
    "    else:\n",
    "        print(\"No results found\")\n",
    "\n",
    "\n",
    "# To run this in a notebook or async environment:\n",
    "df = await main(addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the geocoded addresses to a CSV file\n",
    "df.to_csv(\"../datasets/private/addresses.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
